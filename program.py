# ===========================================
# ETAPA 1: Instalar depend√™ncias (execute apenas 1x)
# ===========================================
# pip install pytesseract pdf2image spacy nltk openai pillow PyPDF2
# python -m spacy download pt_core_news_sm

# ===========================================
# ETAPA 2: Importar bibliotecas
# ===========================================
import pytesseract
from PIL import Image
import spacy
import nltk
import re
import io
import openai
from collections import Counter
from pdf2image import convert_from_path
from PyPDF2 import PdfReader
from nltk.corpus import stopwords
from pathlib import Path

# Baixar stopwords do NLTK (caso n√£o tenha)
nltk.download('stopwords')

# ===========================================
# ETAPA 3: Selecionar arquivo manualmente
# ===========================================
print("üìÅ Informe o caminho completo do arquivo PDF ou imagem:")
file_path = input("üëâ Caminho: ").strip()
file = Path(file_path)

if not file.exists():
    raise FileNotFoundError("Arquivo n√£o encontrado. Verifique o caminho informado.")

print(f"üìÑ Arquivo selecionado: {file.name}")

# ===========================================
# ETAPA 4: Fun√ß√£o de extra√ß√£o de texto
# ===========================================
def extract_text(file_path):
    text = ""
    if file_path.suffix.lower() == ".pdf":
        try:
            reader = PdfReader(file_path)
            for page in reader.pages:
                text += page.extract_text() or ""
            
            # Caso o PDF seja s√≥ imagem (OCR)
            if len(text.strip()) < 100:
                pages = convert_from_path(file_path)
                for page in pages:
                    text += pytesseract.image_to_string(page, lang="por")
        except:
            pages = convert_from_path(file_path)
            for page in pages:
                text += pytesseract.image_to_string(page, lang="por")

    elif file_path.suffix.lower() in [".png", ".jpg", ".jpeg"]:
        image = Image.open(file_path)
        text = pytesseract.image_to_string(image, lang="por")

    return text

texto = extract_text(file)
print("üßæ Texto extra√≠do (primeiros 500 caracteres):\n")
print(texto[:500])

# ===========================================
# ETAPA 5: Detec√ß√£o de artigo cient√≠fico
# ===========================================
def is_scientific_article(text):
    keywords = [
        "resumo", "abstract", "introdu√ß√£o",
        "metodologia", "resultados", "conclus√£o",
        "refer√™ncias", "bibliografia"
    ]
    text_lower = text.lower()
    hits = sum(1 for word in keywords if word in text_lower)
    score = hits / len(keywords)
    return score >= 0.4

eh_artigo = is_scientific_article(texto)
if not eh_artigo:
    print("üö´ Documento inv√°lido: O documento n√£o √© um artigo cient√≠fico.")
else:
    print("‚úÖ Documento v√°lido: Artigo cient√≠fico detectado.")

# ===========================================
# ETAPA 6: Segmenta√ß√£o de par√°grafos
# ===========================================
nlp = spacy.load("pt_core_news_sm")

def segment_paragraphs(text):
    paragraphs = [p.strip() for p in re.split(r'\n\s*\n', text) if len(p.strip()) > 0]
    return paragraphs

paragrafos = segment_paragraphs(texto)
print(f"üß© Total de par√°grafos detectados: {len(paragrafos)}")

# ===========================================
# ETAPA 7: Contagem de palavras e frequ√™ncia
# ===========================================
def analyze_words(paragraphs):
    text = " ".join(paragraphs).lower()
    words = re.findall(r'\b\w+\b', text)
    stop = set(stopwords.words('portuguese'))
    words_clean = [w for w in words if w not in stop and len(w) > 2]
    freq = Counter(words_clean).most_common(10)
    return len(words_clean), freq

total_palavras, freq = analyze_words(paragrafos)
print(f"üìÑ Total de palavras: {total_palavras}")
print("üî† Palavras mais frequentes:", freq)

# ===========================================
# ETAPA 8: Resumo com LLM (somente aqui)
# ===========================================
openai.api_key = "SUA_CHAVE_API_AQUI"  # substitua por sua chave

def gerar_resumo(texto, total_palavras, total_paragrafos):
    regra = (total_palavras > 2000 and total_paragrafos > 4)
    conformidade = "conforme" if regra else "N√ÉO conforme"
    prompt = f"""
    Resuma o seguinte texto em portugu√™s e indique se ele est√° conforme √† regra (>2000 palavras e >4 par√°grafos):
    Texto: {texto[:4000]}...
    Resuma em 4 linhas e finalize dizendo se o texto est√° {conformidade}.
    """
    response = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.4,
    )
    return response.choices[0].message.content

if not eh_artigo:
    resumo = gerar_resumo(texto, total_palavras, len(paragrafos))
    print("\nüìù Resumo Avaliativo:\n", resumo)
# ===========================================

def main():
    try:
        # Solicita caminho do arquivo
        print("üìÅ Informe o caminho completo do arquivo PDF ou imagem:")
        file_path = input("üëâ Caminho: ").strip()
        file = Path(file_path)

        if not file.exists():
            raise FileNotFoundError("Arquivo n√£o encontrado. Verifique o caminho informado.")

        print(f"üìÑ Arquivo selecionado: {file.name}")

        # Extrai texto do documento
        texto = extract_text(file)
        print("üßæ Texto extra√≠do (primeiros 500 caracteres):\n")
        print(texto[:500])

        # Verifica se √© artigo cient√≠fico
        eh_artigo = is_scientific_article(texto)
        if not eh_artigo:
            print("üö´ Documento inv√°lido: O documento n√£o √© um artigo cient√≠fico.")
            return

        print("‚úÖ Documento v√°lido: Artigo cient√≠fico detectado.")

        # Segmenta par√°grafos usando t√©cnica de segmenta√ß√£o
        paragrafos = segment_paragraphs(texto)
        total_paragrafos = len(paragrafos)
        print(f"üß© Total de par√°grafos detectados: {total_paragrafos}")

        # An√°lise de palavras e frequ√™ncia
        total_palavras, palavras_frequentes = analyze_words(paragrafos)
        print(f"üìÑ Total de palavras: {total_palavras}")
        print("üî† Palavras mais frequentes:")
        for palavra, freq in palavras_frequentes:
            print(f"- {palavra}: {freq} ocorr√™ncias")

        # Verifica conformidade com as regras
        esta_conforme = total_palavras > 2000 and total_paragrafos > 4
        status = "conforme" if esta_conforme else "N√ÉO conforme"
        
        # Gera resumo usando LLM apenas nesta etapa
        resumo = gerar_resumo(texto, total_palavras, total_paragrafos)
        print("\nüìù Resumo Avaliativo:\n", resumo)

    except Exception as e:
        print(f"‚ùå Erro ao processar o documento: {str(e)}")

if __name__ == "__main__":
    main()
